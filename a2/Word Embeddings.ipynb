{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "punctuation_symbols = set(string.punctuation)\n",
    "punctuation_symbols.add(\"``\")\n",
    "punctuation_symbols.add(\"''\")\n",
    "punctuation_symbols.add('\"\"')\n",
    "\n",
    "def preprocess_clean(text):\n",
    "    return [w.lower() for w in text if w not in punctuation_symbols]\n",
    "\n",
    "def build_word_id_dict(word_seq, cutoff=5):\n",
    "    counts = Counter(word_seq)\n",
    "    frequent_enough = (word for word in counts if counts[word] >= cutoff)\n",
    "    return dict((word, w_id) for w_id, word in enumerate(frequent_enough))\n",
    "\n",
    "def convert_to_ids(word_seq, word_id_dict):\n",
    "    return np.array([word_id_dict[word] for word in word_seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from fuel.datasets import Dataset\n",
    "\n",
    "class W2VecDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, sources, indexable, context_len):\n",
    "        if len(sources) < 2:\n",
    "            raise ValueError(\"Can't handle more than 2 sources atm\")\n",
    "        \n",
    "        # must be len 2\n",
    "        self.provides_sources = sources\n",
    "        # must be a numpy array\n",
    "        self.indexable = indexable\n",
    "        self.N = context_len\n",
    "        self.axis_labels = None        \n",
    "    \n",
    "    @property\n",
    "    def example_indices(self):\n",
    "        return list(range(self.N, len(self.indexable) - self.N))\n",
    "    \n",
    "    @property\n",
    "    def num_examples(self):\n",
    "        return len(self.example_indices)\n",
    "    \n",
    "    def get_data(self, state=None, request=None):\n",
    "        if state is not None or request is None:\n",
    "            raise ValueError\n",
    "        \n",
    "        by_item = map(self._get_items, request)\n",
    "        contexts, targets = tuple(zip(*by_item))\n",
    "        return (np.array(contexts), np.array(targets))\n",
    "        \n",
    "    def _get_items(self, index):\n",
    "        context_indeces = np.array(range(index - self.N, index + self.N + 1))\n",
    "        # remove the index itself from context indices\n",
    "        context_indeces = context_indeces[context_indeces != index]\n",
    "        try:\n",
    "            return (self.indexable[context_indeces], self.indexable[index])\n",
    "        except IndexError:\n",
    "            raise IndexError(\"{0}, {1}\".format(str(context_indeces), str(index)))\n",
    "\n",
    "def make_w2vec_dataset(indexable_seq):\n",
    "    return W2VecDataset(('contexts', 'targets'), indexable_seq, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corp_len = 10000\n",
    "training_len = 4600\n",
    "wrds = brown.words()[:corp_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_words = preprocess_clean(wrds)\n",
    "\n",
    "vocab = build_word_id_dict(clean_words)\n",
    "\n",
    "filtered_words = [w for w in clean_words if w in vocab]\n",
    "\n",
    "word_ids = convert_to_ids(filtered_words, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_dataset = make_w2vec_dataset(word_ids[:training_len])\n",
    "test_dataset = make_w2vec_dataset(word_ids[training_len:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1081"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset.indexable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1078"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.example_indices[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([130,  80, 248])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset.indexable[4597:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4600"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_dataset.indexable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[1 2 3]'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(np.array([1,2,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Network Definition\n",
    "\n",
    "Revisit regularization: how does it work?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Theano stuff\n",
    "from theano import tensor as T\n",
    "\n",
    "# All sorts of bricks\n",
    "from blocks.bricks.lookup import LookupTable\n",
    "from blocks.bricks import Linear, Softmax\n",
    "from blocks.graph import ComputationGraph\n",
    "from blocks.initialization import IsotropicGaussian, Constant\n",
    "from blocks.algorithms import GradientDescent, Scale\n",
    "from blocks.bricks.cost import CategoricalCrossEntropy, MisclassificationRate\n",
    "# Data Streams and monitoring\n",
    "from fuel.streams import DataStream\n",
    "from fuel.schemes import SequentialScheme, SequentialExampleScheme\n",
    "from blocks.extensions.monitoring import DataStreamMonitoring\n",
    "# Main Loop\n",
    "from blocks.main_loop import MainLoop\n",
    "from blocks.extensions import FinishAfter, Printing\n",
    "from blocks_extras.extensions.plot import Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------------------------------------------------\n",
      "BEFORE FIRST EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 0\n",
      "\t received_first_batch: False\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 0:\n",
      "\t test_x_entropy_cost_apply_cost: 5.717001761934335\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 1\n",
      "\t iterations_done: 230\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 230:\n",
      "\t test_x_entropy_cost_apply_cost: 5.587235289088982\n",
      "\t training_finish_requested: True\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "TRAINING HAS BEEN FINISHED:\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 1\n",
      "\t iterations_done: 230\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 230:\n",
      "\t test_x_entropy_cost_apply_cost: 5.587235289088982\n",
      "\t training_finish_requested: True\n",
      "\t training_finished: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 100\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Network layers\n",
    "# Not sure I should keep this as input?\n",
    "input_layer = T.imatrix('contexts')\n",
    "input_to_projection = LookupTable(vocab_size,\n",
    "                                    hidden_size,\n",
    "                                    weights_init=IsotropicGaussian(0.01),\n",
    "                                    biases_init=Constant(0),\n",
    "                                    name=\"projection\")\n",
    "projection_layer = T.mean(input_to_projection.apply(input_layer), axis=1)\n",
    "projection_layer.name = 'projection'\n",
    "projection_to_ouput = Linear(name='output',\n",
    "                              weights_init=IsotropicGaussian(0.01),\n",
    "                              biases_init=Constant(0),\n",
    "                              input_dim=hidden_size,\n",
    "                              output_dim=vocab_size)\n",
    "probs = Softmax().apply(projection_to_ouput.apply(projection_layer))\n",
    "\n",
    "# Cost Function, Graph\n",
    "true_targets = T.ivector('targets')\n",
    "cost = CategoricalCrossEntropy(name='x_entropy_cost').apply(true_targets, probs)\n",
    "graph = ComputationGraph(cost)\n",
    "\n",
    "# Other metrics\n",
    "# not sure this will work...\n",
    "# error_rate = MisclassificationRate().apply(probs, true_targets)\n",
    "\n",
    "# Parameter Initialization\n",
    "# Idea: annotate layers that need initialization and select them\n",
    "input_to_projection.initialize()\n",
    "projection_to_ouput.initialize()\n",
    "\n",
    "# Cost optimization\n",
    "optimizer = GradientDescent(cost=cost, parameters=graph.parameters,\n",
    "                            step_rule=Scale(learning_rate=0.025))\n",
    "\n",
    "# Data Streams\n",
    "training_stream = DataStream.default_stream(training_dataset,\n",
    "                                            iteration_scheme=SequentialScheme(training_dataset.example_indices, batch_size=20))\n",
    "test_stream = DataStream.default_stream(test_dataset,\n",
    "                                        iteration_scheme=SequentialScheme(test_dataset.example_indices, batch_size=20))\n",
    "# Monitoring\n",
    "monitor = DataStreamMonitoring(variables=[cost], \n",
    "                               data_stream=test_stream, prefix=\"test\")\n",
    "\n",
    "# Main Loop\n",
    "main_loop = MainLoop(data_stream=training_stream, algorithm=optimizer,\n",
    "                     extensions=[monitor,\n",
    "                                     FinishAfter(after_n_epochs=1),\n",
    "                                     Printing(),\n",
    "#                                      Plot(\"Example Plot\", channels=[['test_cost_simple_xentropy', \"test_error_rate\"]])\n",
    "                                ])\n",
    "main_loop.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
