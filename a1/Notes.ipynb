{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception reporting mode: Plain\n",
      "Doctest mode is: ON\n"
     ]
    }
   ],
   "source": [
    "%doctest_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "raw_data = pandas.read_csv(\"agaricus-lepiota.data\", header=None)\n",
    "split = 2031\n",
    "\n",
    "data_one_hot = pandas.get_dummies(raw_data)\n",
    "\n",
    "training_one_hot = (data_one_hot.iloc[split:, 2:], data_one_hot.iloc[split:, :2])\n",
    "\n",
    "testing_one_hot = (data_one_hot.iloc[:split, 2:], data_one_hot.iloc[:split, :2])\n",
    "\n",
    "from fuel.datasets import IndexableDataset\n",
    "training_dataset = IndexableDataset(\n",
    "    indexables={'features': training_one_hot[0].values.astype('i8'), 'targets': training_one_hot[1].values.astype('i8')})\n",
    "testing_dataset = IndexableDataset(\n",
    "    indexables={'features': testing_one_hot[0].values.astype('i8'), 'targets': testing_one_hot[1].values.astype('i8')})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blocks Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "# theano.config.optimizer = \"None\"\n",
    "# theano.config.exception_verbosity = \"high\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ">>> from theano import tensor\n",
    ">>> x = tensor.lmatrix('features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from blocks.bricks import Linear, Logistic, Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    ">>> hidden_layer_size = 100\n",
    ">>> input_to_hidden = Linear(name='input_to_hidden', input_dim=117, output_dim=hidden_layer_size)\n",
    ">>> h = Logistic().apply(input_to_hidden.apply(x))\n",
    ">>> hidden_to_output = Linear(name='hidden_to_output', input_dim=hidden_layer_size, output_dim=2)\n",
    ">>> y_hat = Softmax().apply(hidden_to_output.apply(h))\n",
    "\n",
    ">>> y = tensor.lmatrix('targets')\n",
    ">>> from blocks.bricks.cost import CategoricalCrossEntropy, MisclassificationRate\n",
    ">>> cost = CategoricalCrossEntropy().apply(y, y_hat)\n",
    ">>> error_rate = MisclassificationRate().apply(y.argmax(axis=1), y_hat)\n",
    ">>> error_rate.name = \"error_rate\"\n",
    "\n",
    "# >>> from blocks.roles import WEIGHT\n",
    ">>> from blocks.graph import ComputationGraph\n",
    "# >>> from blocks.filter import VariableFilter\n",
    ">>> cg = ComputationGraph(cost)\n",
    "# >>> W1, W2 = VariableFilter(roles=[WEIGHT])(cg.variables)\n",
    "# >>> cost = cost + 0.005 * (W1 ** 2).sum() + 0.005 * (W2 ** 2).sum()\n",
    "# >>> cost.name = 'cost_with_regularization'\n",
    ">>> cost.name = 'cost_simple_xentropy'\n",
    "\n",
    ">>> from blocks.initialization import IsotropicGaussian, Constant\n",
    ">>> input_to_hidden.weights_init = hidden_to_output.weights_init = IsotropicGaussian(0.01)\n",
    ">>> input_to_hidden.biases_init = hidden_to_output.biases_init = Constant(0)\n",
    ">>> input_to_hidden.initialize()\n",
    ">>> hidden_to_output.initialize()\n",
    "\n",
    ">>> from fuel.streams import DataStream\n",
    ">>> from fuel.schemes import SequentialScheme, SequentialExampleScheme\n",
    "# >>> from fuel.transformers import Flatten\n",
    ">>> data_stream = DataStream.default_stream(\n",
    "...     training_dataset,\n",
    "...     iteration_scheme=SequentialScheme(training_dataset.num_examples, batch_size=20))\n",
    "\n",
    ">>> data_stream_test = DataStream.default_stream(\n",
    "...     testing_dataset,\n",
    "...     iteration_scheme=SequentialScheme(testing_dataset.num_examples, batch_size=split))\n",
    "\n",
    ">>> from blocks.extensions.monitoring import DataStreamMonitoring\n",
    ">>> monitor = DataStreamMonitoring(\n",
    "...     variables=[cost, error_rate], data_stream=data_stream_test, prefix=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things I have tried:\n",
    "\n",
    "- cost\n",
    "- training_cost\n",
    "- train_cost\n",
    "- training_dataset_cost\n",
    "- cost_simple_xentropy\n",
    "- training_cost_simple_xentropy\n",
    "- train_cost_simple_xentropy\n",
    "- training_dataset_cost_simple_xentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using saved session configuration for http://localhost:5006/\n",
      "To override, pass 'load_from_config=False' to Session\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "BEFORE FIRST EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 0\n",
      "\t received_first_batch: False\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 0:\n",
      "\t test_cost_simple_xentropy: 0.6839752156045804\n",
      "\t test_error_rate: 0.11816838995568685\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 1\n",
      "\t iterations_done: 305\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 305:\n",
      "\t test_cost_simple_xentropy: 0.6233611621447643\n",
      "\t test_error_rate: 0.11816838995568685\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 2\n",
      "\t iterations_done: 610\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 610:\n",
      "\t test_cost_simple_xentropy: 0.6165014315330757\n",
      "\t test_error_rate: 0.11816838995568685\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 3\n",
      "\t iterations_done: 915\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 915:\n",
      "\t test_cost_simple_xentropy: 0.5724967782954767\n",
      "\t test_error_rate: 0.11767602166420482\n",
      "\t training_finish_requested: True\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "TRAINING HAS BEEN FINISHED:\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 3\n",
      "\t iterations_done: 915\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 915:\n",
      "\t test_cost_simple_xentropy: 0.5724967782954767\n",
      "\t test_error_rate: 0.11767602166420482\n",
      "\t training_finish_requested: True\n",
      "\t training_finished: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from blocks.algorithms import GradientDescent, Scale\n",
    "algorithm = GradientDescent(cost=cost, parameters=cg.parameters,\n",
    "                            step_rule=Scale(learning_rate=0.025))\n",
    "\n",
    "from blocks.main_loop import MainLoop\n",
    "from blocks.extensions import FinishAfter, Printing\n",
    "from blocks_extras.extensions.plot import Plot\n",
    "main_loop = MainLoop(data_stream=data_stream, algorithm=algorithm,\n",
    "                     extensions=[monitor,\n",
    "                                     FinishAfter(after_n_epochs=3),\n",
    "                                     Printing(),\n",
    "                                     Plot(\"Example Plot\", channels=[['test_cost_simple_xentropy', \"test_error_rate\"]])\n",
    "                                ])\n",
    "main_loop.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "\n",
    "**VERY IMPORTANT**\n",
    "there's some sort of shared state going on in the model definition, so it's important to rerun all the code from the beginning, not just the main loop!\n",
    "\n",
    "Setting the hidden layer to 50 lowered the cost (0.69), but didn't improve the score after more training.\n",
    "Setting the hidden layer to 300 bumped up the cost (1.79), but training made significant improvements in it after first epoch, but not in subsequent ones (more incremental after that).\n",
    "\n",
    "Somehow the total number of epochs influences the starting cost??\n",
    "\n",
    "- epochs: 5 vs 3\n",
    "- learning rate: 0.5\n",
    "- hidden layer: 300\n",
    "\n",
    "When I took the same parameters (5 epochs) and set hidden layer to 100, I got the following progression of costs:\n",
    "\n",
    "- epochs done: 0 = 0.6931921183574188\n",
    "- epochs done: 1 = 1.814269964941275\n",
    "- epochs done: 2 = 1.194301165186615\n",
    "- epochs done: 3 = 0.8182568883881371\n",
    "- epochs done: 4 = 0.7323559855023634\n",
    "- epochs done: 5 = 0.6993469372860405\n",
    "\n",
    "\n",
    "### Data Processing\n",
    "\n",
    "What is `Flatten` for?\n",
    "\n",
    "### Minibatches and Train/Test Split\n",
    "\n",
    "What's the relationship between the test/training data and the minibatch size?\n",
    "Does the batch size have to \"fit\" exactly into the dataset sizes?\n",
    "Why are we also iterating over the test data?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Theano intro tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "from theano import tensoror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = tensor.dscalar(\"a\")\n",
    "b = tensor.dscalar(\"b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = a + b\n",
    "f = theano.function([a, b], c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert 4 == f(1.5, 2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(a + b)'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theano.pp(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Elemwise{add,no_inplace}'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.owner.op.name"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
