{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception reporting mode: Plain\n",
      "Doctest mode is: ON\n"
     ]
    }
   ],
   "source": [
    "%doctest_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "raw_data = pandas.read_csv(\"agaricus-lepiota.data\", header=None)\n",
    "split = 2031\n",
    "\n",
    "data_one_hot = pandas.get_dummies(raw_data)\n",
    "\n",
    "training_one_hot = (data_one_hot.iloc[split:, 2:], data_one_hot.iloc[split:, :2])\n",
    "\n",
    "testing_one_hot = (data_one_hot.iloc[:split, 2:], data_one_hot.iloc[:split, :2])\n",
    "\n",
    "from fuel.datasets import IndexableDataset\n",
    "training_dataset = IndexableDataset(\n",
    "    indexables={'features': training_one_hot[0].values.astype('i8'), 'targets': training_one_hot[1].values.astype('i8')})\n",
    "testing_dataset = IndexableDataset(\n",
    "    indexables={'features': testing_one_hot[0].values.astype('i8'), 'targets': testing_one_hot[1].values.astype('i8')})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blocks Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "# theano.config.optimizer = \"None\"\n",
    "# theano.config.exception_verbosity = \"high\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ">>> from theano import tensor\n",
    ">>> x = tensor.lmatrix('features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from blocks.bricks import Linear, Logistic, Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    ">>> hidden_layer_size = 100\n",
    ">>> input_to_hidden = Linear(name='input_to_hidden', input_dim=117, output_dim=hidden_layer_size)\n",
    ">>> h = Logistic().apply(input_to_hidden.apply(x))\n",
    ">>> hidden_to_output = Linear(name='hidden_to_output', input_dim=hidden_layer_size, output_dim=2)\n",
    ">>> y_hat = Softmax().apply(hidden_to_output.apply(h))\n",
    "\n",
    ">>> y = tensor.lmatrix('targets')\n",
    ">>> from blocks.bricks.cost import CategoricalCrossEntropy, MisclassificationRate\n",
    ">>> cost = CategoricalCrossEntropy().apply(y, y_hat)\n",
    ">>> error_rate = MisclassificationRate().apply(y.argmax(axis=1), y_hat)\n",
    ">>> error_rate.name = \"error_rate\"\n",
    "\n",
    "# >>> from blocks.roles import WEIGHT\n",
    ">>> from blocks.graph import ComputationGraph\n",
    "# >>> from blocks.filter import VariableFilter\n",
    ">>> cg = ComputationGraph(cost)\n",
    "# >>> W1, W2 = VariableFilter(roles=[WEIGHT])(cg.variables)\n",
    "# >>> cost = cost + 0.005 * (W1 ** 2).sum() + 0.005 * (W2 ** 2).sum()\n",
    "# >>> cost.name = 'cost_with_regularization'\n",
    ">>> cost.name = 'cost_simple_xentropy'\n",
    "\n",
    ">>> from blocks.initialization import IsotropicGaussian, Constant\n",
    ">>> input_to_hidden.weights_init = hidden_to_output.weights_init = IsotropicGaussian(0.01)\n",
    ">>> input_to_hidden.biases_init = hidden_to_output.biases_init = Constant(0)\n",
    ">>> input_to_hidden.initialize()\n",
    ">>> hidden_to_output.initialize()\n",
    "\n",
    ">>> from fuel.streams import DataStream\n",
    ">>> from fuel.schemes import SequentialScheme, SequentialExampleScheme\n",
    "# >>> from fuel.transformers import Flatten\n",
    ">>> data_stream = DataStream.default_stream(\n",
    "...     training_dataset,\n",
    "...     iteration_scheme=SequentialScheme(training_dataset.num_examples, batch_size=20))\n",
    "\n",
    ">>> data_stream_test = DataStream.default_stream(\n",
    "...     testing_dataset,\n",
    "...     iteration_scheme=SequentialScheme(testing_dataset.num_examples, batch_size=split))\n",
    "\n",
    ">>> from blocks.extensions.monitoring import DataStreamMonitoring\n",
    ">>> monitor = DataStreamMonitoring(\n",
    "...     variables=[cost, error_rate], data_stream=data_stream_test, prefix=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DataStreamMonitoring?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorType(float64, scalar)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using saved session configuration for http://localhost:5006/\n",
      "To override, pass 'load_from_config=False' to Session\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quickbeam/ilia_dev/python/miniconda/envs/py3-deep-learning/lib/python3.5/site-packages/bokeh/session.py:318: UserWarning: You need to start the bokeh-server to see this example.\n",
      "  warnings.warn(\"You need to start the bokeh-server to see this example.\")\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "HTTPConnectionPool(host='localhost', port=5006): Max retries exceeded with url: /bokeh/userinfo/ (Caused by NewConnectionError('<requests.packages.urllib3.connection.HTTPConnection object at 0xa6a27a6c>: Failed to establish a new connection: [Errno 111] Connection refused',))",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m\"<ipython-input-7-d7de8104f107>\"\u001b[0m, line \u001b[0;32m12\u001b[0m, in \u001b[0;35m<module>\u001b[0m\n    Plot(\"Example Plot\", channels=[['test_cost_simple_xentropy', \"test_error_rate\"]])\n",
      "  File \u001b[0;32m\"/home/quickbeam/ilia_dev/python/miniconda/envs/py3-deep-learning/lib/python3.5/site-packages/blocks_extras-0.0.0-py3.5.egg/blocks_extras/extensions/plot.py\"\u001b[0m, line \u001b[0;32m189\u001b[0m, in \u001b[0;35m__init__\u001b[0m\n    super(Plot, self).__init__(document_name, **kwargs)\n",
      "  File \u001b[0;32m\"/home/quickbeam/ilia_dev/python/miniconda/envs/py3-deep-learning/lib/python3.5/site-packages/blocks_extras-0.0.0-py3.5.egg/blocks_extras/extensions/plot.py\"\u001b[0m, line \u001b[0;32m63\u001b[0m, in \u001b[0;35m__init__\u001b[0m\n    self._setup_document(clear_document)\n",
      "  File \u001b[0;32m\"/home/quickbeam/ilia_dev/python/miniconda/envs/py3-deep-learning/lib/python3.5/site-packages/blocks_extras-0.0.0-py3.5.egg/blocks_extras/extensions/plot.py\"\u001b[0m, line \u001b[0;32m79\u001b[0m, in \u001b[0;35m_setup_document\u001b[0m\n    self.session.use_doc(self.document_name)\n",
      "  File \u001b[0;32m\"/home/quickbeam/ilia_dev/python/miniconda/envs/py3-deep-learning/lib/python3.5/site-packages/bokeh/session.py\"\u001b[0m, line \u001b[0;32m455\u001b[0m, in \u001b[0;35muse_doc\u001b[0m\n    self.docid = self.find_doc(name)\n",
      "  File \u001b[0;32m\"/home/quickbeam/ilia_dev/python/miniconda/envs/py3-deep-learning/lib/python3.5/site-packages/bokeh/session.py\"\u001b[0m, line \u001b[0;32m418\u001b[0m, in \u001b[0;35mfind_doc\u001b[0m\n    docs = self.userinfo.get('docs')\n",
      "  File \u001b[0;32m\"/home/quickbeam/ilia_dev/python/miniconda/envs/py3-deep-learning/lib/python3.5/site-packages/bokeh/session.py\"\u001b[0m, line \u001b[0;32m373\u001b[0m, in \u001b[0;35muserinfo\u001b[0m\n    self._userinfo = self.get_json(url)\n",
      "  File \u001b[0;32m\"/home/quickbeam/ilia_dev/python/miniconda/envs/py3-deep-learning/lib/python3.5/site-packages/bokeh/session.py\"\u001b[0m, line \u001b[0;32m351\u001b[0m, in \u001b[0;35mget_json\u001b[0m\n    return self.execute_json('get', url, headers=headers, **kwargs)\n",
      "  File \u001b[0;32m\"/home/quickbeam/ilia_dev/python/miniconda/envs/py3-deep-learning/lib/python3.5/site-packages/bokeh/session.py\"\u001b[0m, line \u001b[0;32m334\u001b[0m, in \u001b[0;35mexecute_json\u001b[0m\n    resp = self.execute(method, url, headers=headers, **kwargs)\n",
      "  File \u001b[0;32m\"/home/quickbeam/ilia_dev/python/miniconda/envs/py3-deep-learning/lib/python3.5/site-packages/bokeh/session.py\"\u001b[0m, line \u001b[0;32m319\u001b[0m, in \u001b[0;35mexecute\u001b[0m\n    raise e\n",
      "  File \u001b[0;32m\"/home/quickbeam/ilia_dev/python/miniconda/envs/py3-deep-learning/lib/python3.5/site-packages/bokeh/session.py\"\u001b[0m, line \u001b[0;32m316\u001b[0m, in \u001b[0;35mexecute\u001b[0m\n    resp = func(url, headers=headers, **kwargs)\n",
      "  File \u001b[0;32m\"/home/quickbeam/ilia_dev/python/miniconda/envs/py3-deep-learning/lib/python3.5/site-packages/requests/sessions.py\"\u001b[0m, line \u001b[0;32m480\u001b[0m, in \u001b[0;35mget\u001b[0m\n    return self.request('GET', url, **kwargs)\n",
      "  File \u001b[0;32m\"/home/quickbeam/ilia_dev/python/miniconda/envs/py3-deep-learning/lib/python3.5/site-packages/requests/sessions.py\"\u001b[0m, line \u001b[0;32m468\u001b[0m, in \u001b[0;35mrequest\u001b[0m\n    resp = self.send(prep, **send_kwargs)\n",
      "  File \u001b[0;32m\"/home/quickbeam/ilia_dev/python/miniconda/envs/py3-deep-learning/lib/python3.5/site-packages/requests/sessions.py\"\u001b[0m, line \u001b[0;32m576\u001b[0m, in \u001b[0;35msend\u001b[0m\n    r = adapter.send(request, **kwargs)\n",
      "\u001b[1;36m  File \u001b[1;32m\"/home/quickbeam/ilia_dev/python/miniconda/envs/py3-deep-learning/lib/python3.5/site-packages/requests/adapters.py\"\u001b[1;36m, line \u001b[1;32m437\u001b[1;36m, in \u001b[1;35msend\u001b[1;36m\u001b[0m\n\u001b[1;33m    raise ConnectionError(e, request=request)\u001b[0m\n",
      "\u001b[1;31mConnectionError\u001b[0m\u001b[1;31m:\u001b[0m HTTPConnectionPool(host='localhost', port=5006): Max retries exceeded with url: /bokeh/userinfo/ (Caused by NewConnectionError('<requests.packages.urllib3.connection.HTTPConnection object at 0xa6a27a6c>: Failed to establish a new connection: [Errno 111] Connection refused',))\n"
     ]
    }
   ],
   "source": [
    "from blocks.algorithms import GradientDescent, Scale\n",
    "algorithm = GradientDescent(cost=cost, parameters=cg.parameters,\n",
    "                            step_rule=Scale(learning_rate=0.025))\n",
    "\n",
    "from blocks.main_loop import MainLoop\n",
    "from blocks.extensions import FinishAfter, Printing\n",
    "from blocks_extras.extensions.plot import Plot\n",
    "main_loop = MainLoop(data_stream=data_stream, algorithm=algorithm,\n",
    "                     extensions=[monitor,\n",
    "                                     FinishAfter(after_n_epochs=3),\n",
    "                                     Printing(),\n",
    "                                     Plot(\"Example Plot\", channels=[['test_cost_simple_xentropy', \"test_error_rate\"]])\n",
    "                                ])\n",
    "main_loop.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "\n",
    "**VERY IMPORTANT**\n",
    "there's some sort of shared state going on in the model definition, so it's important to rerun all the code from the beginning, not just the main loop!\n",
    "\n",
    "Setting the hidden layer to 50 lowered the cost (0.69), but didn't improve the score after more training.\n",
    "Setting the hidden layer to 300 bumped up the cost (1.79), but training made significant improvements in it after first epoch, but not in subsequent ones (more incremental after that).\n",
    "\n",
    "Somehow the total number of epochs influences the starting cost??\n",
    "\n",
    "- epochs: 5 vs 3\n",
    "- learning rate: 0.5\n",
    "- hidden layer: 300\n",
    "\n",
    "When I took the same parameters (5 epochs) and set hidden layer to 100, I got the following progression of costs:\n",
    "\n",
    "- epochs done: 0 = 0.6931921183574188\n",
    "- epochs done: 1 = 1.814269964941275\n",
    "- epochs done: 2 = 1.194301165186615\n",
    "- epochs done: 3 = 0.8182568883881371\n",
    "- epochs done: 4 = 0.7323559855023634\n",
    "- epochs done: 5 = 0.6993469372860405\n",
    "\n",
    "\n",
    "### Data Processing\n",
    "\n",
    "What is `Flatten` for?\n",
    "\n",
    "### Minibatches and Train/Test Split\n",
    "\n",
    "What's the relationship between the test/training data and the minibatch size?\n",
    "Does the batch size have to \"fit\" exactly into the dataset sizes?\n",
    "Why are we also iterating over the test data?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Theano intro tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "from theano import tensoror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = tensor.dscalar(\"a\")\n",
    "b = tensor.dscalar(\"b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = a + b\n",
    "f = theano.function([a, b], c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert 4 == f(1.5, 2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(a + b)'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theano.pp(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Elemwise{add,no_inplace}'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.owner.op.name"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
